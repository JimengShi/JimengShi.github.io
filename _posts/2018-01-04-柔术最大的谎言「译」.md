---
layout:     post
title:      梯度消失、梯度爆炸
subtitle:   梯度消失、梯度爆炸的原因及解决办法
date:       2020-08-14
author:     Jimeng
header-img: img/post-bg-BJJ.jpg
catalog: true
tags:
    - Machine Learning
---

[梯度消失、爆炸的原因及解决办法](https://zhuanlan.zhihu.com/p/180568816)


![梯度消失、爆炸的原因及解决办法](https://zhuanlan.zhihu.com/p/180568816)




## 一、引入：梯度更新规则
目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，更新优化深度网络的权值。这样做是有一定原因的，首先，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 f(x)，因此整个深度网络可以视为是一个复合的非线性多元函数：


我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射。

## 二、梯度消失、爆炸的原因
下图以三个隐层的单神经元网络为例：



假设每一层网络激活后的输出为 [公式] ，其中 i 为第 i 层，x 代表第 i 层的输入，也就是第 i−1 层的输出，f 是激活函数，那么，可得出 [公式] ，暂时忽略常数 b，简记为 [公式] 。BP算法基于梯度下降策略如下：


由上图可知，上式主要由两部分构成：多个激活函数偏导数的连乘，和多个权重参数的连乘。如果激活函数求导后与权重相乘的积大于1，那么随着层数增多，求出的梯度更新信息将以指数形式增加，即发生梯度爆炸；如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生梯度消失。

【注意】具体激活函数的导数情况可自行验证，比如sigmoid，tanh，Relu，leaky-Relu等。

## 三、解决方法
在讨论解决方法之前，我们探讨一下其解决思想，其实就是抑制上述式子连乘后结果远远大于1或小于1，主要取决于激活函数偏导数和权值大小。

### 3.1 预训练加微调

此方法来自Hinton在2006年发表的一篇论文，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training），得到暂时的最优权重值；在预训练完成后，再利用BP算法对整个网络进行训练，对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用不是很多。

### 3.2 梯度剪切、正则

梯度剪切主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，更新梯度时，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内，防止梯度爆炸。另一种解决梯度爆炸的手段是权重正则化（weithts regularization），常见的是 [公式] 正则，和 [公式] 正则。

### 3.3 合理的激活函数+权值初始化

Relu激活函数：思想是，如果激活函数的导数为1，那么就消除了激活函数偏导数的影响，只需考虑权值即可。


### 3.4 Batch Normalization

Batch norm已被广泛应用到各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题，它是将输出信号 x 规范化，以保证网络的稳定性。


As the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again - a problem the authors refer to as "internal covariate shift". By normalizing the data in each mini-batch, this problem is largely avoided.”Internal Covariate Shift：此术语是google小组在论文 Batch Normalization 中提出来的，其主要描述的是：训练深度网络的时候经常发生训练困难的问题，如上图所示，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难，此现象称之为Internal Covariate Shift。

为了解决这个问题，Batch Norm就派上用场了，它应用于每层激活函数之前，就是做均值和方差归一化，对于每一批次数据并且还做放大缩小，平移，为了梯度下降的收敛速度更快，相当于把数据都拉到中间的位置了，有这个就不需要Dropout，Relu等等。BN使得每层输出信号满足均值为0，方差为1的分布，而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入，从而保证整个网络的稳定性。简而言之，BN包括两点：归一化+缩放平移，具体伪代码如下：




为了说明BN如何防止梯度消失，梯度爆炸，进而加快训练速度，举例如下：

1、带有BN的前向传播过程如下所示（其中 [公式] 为列向量， [公式] 为 [公式] ）：

[公式]

2、则其反向传播有：

[公式]

3、相应的，连续多层的梯度反向传播过程为：

[公式]

可以看出，与不使用BN相比，每层的反向传播过程的，增加了一个基于标准差的矩阵 [公式] 对权重 [公式] 进行缩放，这样的缩放能够产生什么效果？如果权重 [公式] 较小，那必然 [公式] 较小，从而使得其标准差 [公式] 较小，相对的 [公式] 较大，所以 [公式] 相对于原本的 [公式] 就放大了，避免了梯度的衰减；同样的，如果权重 [公式] 较大，可以很容易得到 [公式] 相对于原本的 [公式] 缩小了，避免了梯度的膨胀。于是，加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。

最后直观感受一下有无BN的训练过程：


### 3.5 残差结构

论文参考：Deep Residual Learning for Image Recognition，详细解读可以参考知乎链接：https://zhuanlan.zhihu.com/p/31852747。

