---
layout:     post
title:      Data Pre-processing Methods
subtitle:   Data cleaning, data integration, data transformation, data normalization
date:       2020-09-07
author:     Jimeng
header-img: img/post-bg-BJJ.jpg
catalog: true
tags:
    - Machine Learning
---

	
[Chinese Version](https://zhuanlan.zhihu.com/p/180568816)


## 1 Introduction
In engineering practice, the data we get may contain a large number of missing values, repeated values, outliers, etc., and a large amount of noise. It may also be that there are outliers due to manual input errors, which is not conducive to the training of algorithm model. For the task of data preprocessing, it is commonly said that there are four steps: data cleaning, data integration, data transformation and data normalization.


## 2 Data Cleansing
#### 2.1 Remove unique attributes
The unique attributes are usually id attributes, which cannot describe the distribution rule of the sample itself, so they can be deleted.

#### 2.2 Handle missing values
- Deletion of features containing missing values: it is applicable to an attribute containing a large number of missing values, with the missing rate greater than 80%;
Missing value filling: based on data distribution, modeling prediction, interpolation, multiple interpolation, maximum likelihood estimation, compression perception and matrix completion.
- Fill according to data distribution: if the data conforms to uniform distribution, fill the gap with the mean value of the variable or the homogeneous mean value of the variable; if the data has skewed distribution, fill the gap with the median.
Modeling prediction filling: the missing attribute is predicted as the prediction target, the data set is divided into two categories according to whether the missing value contains a specific attribute, and the missing value of the prediction data set is predicted by using the existing machine learning algorithm (regression, Bayesian, random forest, decision tree and other models).(The fundamental flaw in this approach is that the predicted result is meaningless if the other attributes are unrelated to the missing attribute;However, if the prediction result is quite accurate, it shows that the missing attribute is not necessary to be included in the data set.The general case is somewhere in between.)
- Interpolation filling: including random interpolation, multiple interpolation, Lagrange interpolation, Newton interpolation, manual filling and so on.
Multiple interpolation considers that the value to be interpolated is random. In practice, it usually estimates the value to be interpolated and adds different noises to form multiple groups of alternative interpolation values. According to some selection basis, the most appropriate interpolation value is selected.
The interpolation process only supplements the unknown value with our subjective estimate, which does not necessarily conform to the objective fact.In many cases, it is better to manually interpolate missing values based on your understanding of the domain.
To sum up, the missing proportion of the variable is first detected by using Quant.isnull.sum (), and then deletion or filling is considered. If the variable to be filled is continuous, mean method and interpolation method are generally used for filling; if the variable is discrete, median or modeling prediction are usually used for filling.

#### 2.3 Outlier handling
Outliers are the normal distribution of data, and data outside a particular distribution area or range is usually defined as abnormal or noisy.Exceptions can be divided into two types: "pseudo-exceptions", which are generated by specific business operation actions and normally reflect the state of the business rather than the anomaly of the data itself;"True anomalies" are not caused by specific business operations, but rather anomalies in the distribution of data, namely outliers.
Outlier detection method:
Based on simple statistical analysis: determine whether there are exceptions based on boxplot and each point. For example, Pandas' Describe function can quickly find outliers.
Based on the principle of 3 [Formula] : If the data has a normal distribution and is far from the mean value, the points within the range of [formula] are generally defined as outliers.
Based on median absolute deviation: This is a robust distance value method against outliers. The influence of outliers is amplified by calculating the sum of the distance between each observed value and the average value.
Based on distance: By defining the proximity measure between objects, the exception object can be judged according to the distance whether it is far away from other objects. The disadvantage is that the computational complexity is high and it is not suitable for large data sets and data sets with different density regions
Density-based: The local density of outliers is significantly lower than that of most neighboring points, which is suitable for non-uniform data sets
Clustering based: The clustering algorithm is used to discard the small clusters that are far away from other clusters.
Means of outlier treatment:
Considering whether to delete this record according to the number and impact of outliers, information loss is high
If the outliers are eliminated after the log-scale logarithmic transformation of the data, then this method is effective without loss of information
The mean or median replaces the outlier, which is simple and efficient with less loss of information
In the training of tree model, the tree model has higher robustness to outliers without information loss and does not affect the training effect of the model


