---
layout:     post
title:      Data Pre-processing Methods
subtitle:   Data cleaning, data integration, data transformation, data normalization
date:       2020-09-07
author:     Jimeng
header-img: img/southeast_view.jpg
catalog: true
tags:
    - Machine Learning
---

	
[Chinese Version](https://zhuanlan.zhihu.com/c_1271766060510449664)


## 1 Introduction
In engineering practice, the data we get may contain a large number of **missing values, repeated values, outliers, etc., and a large amount of noise**. It may also be that there are outliers due to manual input errors, which is not conducive to train of algorithm model. For the task of data preprocessing, it is commonly said that there are 4 steps: 

- data cleaning
- data integration
- data transformation
- data normalization


## 2 Data Cleansing
### 2.1 Remove unique attributes
The unique attributes are usually id attributes, which cannot describe the distribution of the sample, so they can be deleted.

### 2.2 Handle missing values
- *Delete those features containing missing values:* it is applicable to an attribute containing a large number of missing values, with the missing rate greater than 80%.

- *Missing value filling:* based on data distribution, modeling prediction, interpolation, maximum likelihood estimation.

    - `Fill according to data distribution:` if the data conforms to uniform distribution, fill the gap with the mean value of the variable or the homogeneous mean value of the variable; if the data has skewed distribution, fill the gap with the median.
    
    - `Modeling prediction filling:` the data set is divided into two categories according to whether the missing value contains a specific attribute, the missing attribute is predicted as the prediction target, then the missing value can be predicted by using the existing machine learning algorithm (regression, Bayesian, random forest, decision tree and other models). (The flaw of this approach is that the predicted result is meaningless if the other attributes are unrelated to the missing attribute; However, if the prediction result is quite accurate, it shows that the missing attribute is not necessary to be included in the data set because their high relationship. The general case is somewhere in between the above two cases.)
    
    - `Interpolation filling:` including random interpolation, multiple interpolation, Lagrange interpolation, Newton interpolation, manual filling and so on.
        - Multiple interpolation considers that the value to be interpolated is random. In practice, it usually estimates the value to be interpolated and adds different noises to form multiple groups of alternative interpolation values. According to some selection basis, the most appropriate interpolation value is selected.
        - The interpolation process only supplements the unknown value with our subjective estimate, which does not necessarily conform to the objective fact.In many cases, it is better to manually interpolate missing values based on your understanding of the domain.

To sum up, the missing proportion of the variable is first detected by using pandas.isnull.sum(), and then deletion or filling is considered. If the variable to be filled is continuous, mean method and interpolation method are generally used for filling; if the variable is discrete, median or modeling prediction are usually used for filling.

### 2.3 Outlier handling
Outliers are usually defined as abnormal or noisy, which refers to those data outside a particular distribution range. Exceptions can be divided into two types: "pseudo-exceptions", which are generated by specific business operation actions and normally reflect the state of the business rather than the anomaly of the data itself; "True anomalies" are not caused by specific business operations, but rather anomalies in the distribution of data, namely outliers.

#### Outlier detection method:
- *Based on simple statistical analysis:* determine whether there are exceptions based on boxplot and each point. For example, "Describe function" in Pandas can quickly find outliers.

- Based on the principle of $ 3 \sigma$: If the data has a normal distribution and is far from the mean value, the points within the range of $P(|x-u| > 3 \sigma ) <= 0.003$ are generally defined as outliers.

- *Based on distance:* By defining the proximity measure between objects, the exception object can be judged according to the distance whether it is far away from other objects. The disadvantage is that the computational complexity is high and it is not suitable for large data sets and data sets with different density regions.

- *Density-based:* The local density of outliers is significantly lower than that of most neighbor points, which is suitable for non-uniform data sets.

- *Clustering based:* The clustering algorithm is used to discard the small clusters that are far away from other clusters.

#### Means of outlier treatment
- Considering whether to delete this record according to the number and impact of outliers, information loss is high.

- If the outliers are eliminated after the log-scale logarithmic transformation of the data, then this method is effective without loss of information.

- The mean or median replaces the outlier, which is simple and efficient with less loss of information.

- In the training of tree model, the tree model has higher robustness to outliers without information loss and does not affect the training effect of the model.




